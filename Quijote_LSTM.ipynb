{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import codecs\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hay 400000 caracteres, 77 unicos.\n"
     ]
    }
   ],
   "source": [
    "data_file = 'data/quijote.txt' # Quijote file in plain text\n",
    "\n",
    "# Reading \"El quijote\" with the codecs\n",
    "data = codecs.open(data_file, 'rU', 'utf-8').read() \n",
    "data = data [300000:700000]\n",
    "chars = list(set(data))\n",
    "data_size, vocab = len(data), len(chars)\n",
    "print 'Hay {} caracteres, {} unicos.'.format(data_size, vocab)\n",
    "\n",
    "# Mini methods to change from character to index and the opposite\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "# hyperparameters\n",
    "seq_length = 10 # number of steps to unroll the RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n, p = 0, 0\n",
    "n_max = 100000\n",
    "inputs = []\n",
    "targets = []\n",
    "\n",
    "while n <= n_max:\n",
    "    \n",
    "    if p + seq_length + 1 >= len(data) or n==0:\n",
    "        p = 0\n",
    "    input = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "    target = [char_to_ix[ch] for ch in data[p+seq_length:p+seq_length+1]]\n",
    "    p +=1\n",
    "    n +=1\n",
    "    inputs.append(input)\n",
    "    targets.append(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Target to one hot\n",
    "one_hot_targets = []\n",
    "for target in targets:\n",
    "    _target = target[0]\n",
    "    one_hot_target = np.zeros(vocab)\n",
    "    one_hot_target[_target] = 1\n",
    "    one_hot_targets.append(one_hot_target)\n",
    "targets = np.array(one_hot_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inputs to one hot\n",
    "one_hot_inputs = []\n",
    "for input in inputs:\n",
    "    one_hot_input = []\n",
    "    for value in input:\n",
    "        _input = np.zeros(vocab)\n",
    "        _input[value] = 1\n",
    "        one_hot_input.append(_input)\n",
    "    one_hot_input = np.array(one_hot_input)\n",
    "    one_hot_inputs.append(one_hot_input)\n",
    "inputs = np.array(one_hot_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100001, 10, 77)\n",
      "(100001, 77)\n"
     ]
    }
   ],
   "source": [
    "input_data = np.array(inputs)\n",
    "output_data = np.array(targets)\n",
    "\n",
    "print input_data.shape\n",
    "print output_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data split\n",
    "\n",
    "LIMIT = 24000\n",
    "X_test = input_data[LIMIT:]\n",
    "Y_test = output_data[LIMIT:]\n",
    "\n",
    "X_train = input_data[:LIMIT]\n",
    "Y_train = output_data[:LIMIT]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Network parameters\n",
    "\n",
    "data = tf.placeholder(tf.float32,[None,seq_length,vocab])\n",
    "target = tf.placeholder(tf.float32,[None,vocab])\n",
    "\n",
    "hidden = 2\n",
    "neurons = 400\n",
    "\n",
    "cell = tf.contrib.rnn.LSTMCell(neurons, state_is_tuple=True)\n",
    "stacked_lstm = tf.contrib.rnn.MultiRNNCell([cell] * hidden, state_is_tuple=True)\n",
    "val, state = tf.nn.dynamic_rnn(stacked_lstm, data, dtype=tf.float32)\n",
    "\n",
    "val = tf.transpose(val, [1, 0, 2])\n",
    "last = tf.gather(val, int(val.get_shape()[0]) - 1)\n",
    "\n",
    "weight = tf.Variable(tf.truncated_normal([neurons, int(target.get_shape()[1])]), name=\"asfsd\")\n",
    "bias = tf.Variable(tf.constant(0.0, shape=[target.get_shape()[1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = tf.matmul(last, weight) + bias\n",
    "prediction = tf.nn.softmax(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/diegoperezsastre/virtualenvs/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/gradients_impl.py:91: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "# Functions definition \n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=target, logits=y))\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.01)\n",
    "minimize = optimizer.minimize(loss)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(prediction, 1), tf.argmax(target, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_op = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24000 60\n"
     ]
    }
   ],
   "source": [
    "batch_size = 400\n",
    "no_of_batches = int(len(X_train) / batch_size)\n",
    "epochs = 4000\n",
    "\n",
    "print len(X_train),no_of_batches\n",
    "loss_train_array = []\n",
    "train_accuracy_array = []\n",
    "test_accuracy_array = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0000, loss train 3.11805105, train accuracy 0.16912501, test accuracy 0.16786622\n",
      "Epoch 0100, loss train 0.19814083, train accuracy 0.93937498, test accuracy 0.39573163\n",
      "Epoch 0200, loss train 0.06234388, train accuracy 0.97750002, test accuracy 0.40011317\n",
      "Epoch 0300, loss train 0.16945450, train accuracy 0.95775002, test accuracy 0.38766596\n",
      "Epoch 0400, loss train 0.10664427, train accuracy 0.97154164, test accuracy 0.39063960\n",
      "Epoch 0500, loss train 0.07879021, train accuracy 0.97662503, test accuracy 0.38891593\n",
      "Epoch 0600, loss train 0.20835577, train accuracy 0.95674998, test accuracy 0.38917908\n",
      "Epoch 0700, loss train 0.13127387, train accuracy 0.94575000, test accuracy 0.37687662\n",
      "Epoch 0800, loss train 0.14614144, train accuracy 0.95291668, test accuracy 0.38348180\n",
      "Epoch 0900, loss train 0.14749897, train accuracy 0.94929165, test accuracy 0.38194233\n",
      "Epoch 1000, loss train 0.10761897, train accuracy 0.96499997, test accuracy 0.38192919\n",
      "Epoch 1100, loss train 0.12344006, train accuracy 0.96433336, test accuracy 0.37553453\n",
      "Epoch 1200, loss train 0.12988789, train accuracy 0.96229166, test accuracy 0.37844238\n",
      "Epoch 1300, loss train 0.27133217, train accuracy 0.94062501, test accuracy 0.37511349\n",
      "Epoch 1400, loss train 0.12697743, train accuracy 0.96841669, test accuracy 0.37937659\n",
      "Epoch 1500, loss train 0.22607449, train accuracy 0.93395835, test accuracy 0.37270561\n",
      "Epoch 1600, loss train 0.12231748, train accuracy 0.96137500, test accuracy 0.37978446\n",
      "Epoch 1700, loss train 0.16295460, train accuracy 0.96100003, test accuracy 0.36773202\n",
      "Epoch 1800, loss train 0.32594627, train accuracy 0.94375002, test accuracy 0.37398192\n",
      "Epoch 1900, loss train 0.16318282, train accuracy 0.96008331, test accuracy 0.37398192\n",
      "Epoch 2000, loss train 0.26770473, train accuracy 0.94266665, test accuracy 0.37427139\n",
      "Epoch 2100, loss train 0.25252599, train accuracy 0.94145834, test accuracy 0.37238982\n",
      "Epoch 2200, loss train 0.13727020, train accuracy 0.96074998, test accuracy 0.37254772\n",
      "Epoch 2300, loss train 0.22788322, train accuracy 0.93500000, test accuracy 0.36990303\n",
      "Epoch 2400, loss train 0.16259052, train accuracy 0.95249999, test accuracy 0.37561348\n",
      "Epoch 2500, loss train 0.11365741, train accuracy 0.96412498, test accuracy 0.37187669\n",
      "Epoch 2600, loss train 0.14376198, train accuracy 0.95745832, test accuracy 0.37266615\n",
      "Epoch 2700, loss train 0.21532227, train accuracy 0.94054168, test accuracy 0.37377140\n",
      "Epoch 2800, loss train 0.55458647, train accuracy 0.87870836, test accuracy 0.37061355\n",
      "Epoch 2900, loss train 0.39873549, train accuracy 0.89454168, test accuracy 0.36831096\n",
      "Epoch 3000, loss train 0.35794970, train accuracy 0.91958332, test accuracy 0.36961356\n",
      "Epoch 3100, loss train 0.39064190, train accuracy 0.91666669, test accuracy 0.36436364\n",
      "Epoch 3200, loss train 0.33000374, train accuracy 0.91787499, test accuracy 0.37167931\n",
      "Epoch 3300, loss train 0.29566899, train accuracy 0.93183333, test accuracy 0.37181088\n",
      "Epoch 3400, loss train 0.24468592, train accuracy 0.93916667, test accuracy 0.37695557\n",
      "Epoch 3500, loss train 0.20118731, train accuracy 0.94933331, test accuracy 0.37610030\n",
      "Epoch 3600, loss train 0.25522807, train accuracy 0.93512499, test accuracy 0.37233719\n",
      "Epoch 3700, loss train 0.36555722, train accuracy 0.92512500, test accuracy 0.36588994\n",
      "Epoch 3800, loss train 0.31067318, train accuracy 0.91562498, test accuracy 0.36999512\n",
      "Epoch 3900, loss train 0.19860631, train accuracy 0.94166666, test accuracy 0.37254772\n",
      "CPU times: user 13d 18h 25min 3s, sys: 1d 51min 37s, total: 14d 19h 16min 41s\n",
      "Wall time: 2d 9h 35min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with sess.as_default():\n",
    "    for i in range(epochs):\n",
    "        ptr = 0\n",
    "        for j in range(no_of_batches):\n",
    "            inp, out = X_train[ptr:ptr + batch_size], Y_train[ptr:ptr + batch_size]\n",
    "            ptr += batch_size\n",
    "            _, loss_train = sess.run([minimize, loss], {data: inp, target: out})\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            train_accuracy = sess.run(\n",
    "                accuracy, feed_dict={data: X_train, target: Y_train})\n",
    "            train_accuracy_array.append(train_accuracy)\n",
    "\n",
    "            test_accuracy = sess.run(\n",
    "                accuracy, feed_dict={data: X_test, target: Y_test})\n",
    "            test_accuracy_array.append(test_accuracy)\n",
    "\n",
    "            print (\n",
    "                'Epoch %04d, '\n",
    "                'loss train %.8f, train accuracy %.8f, test accuracy %.8f'\n",
    "                %\n",
    "                (i,\n",
    "                 loss_train,\n",
    "                 train_accuracy,\n",
    "                 test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sample from the model now and then\n",
    "preds = []\n",
    "ix = X_train[:1]\n",
    "\n",
    "for i in range(1000):\n",
    "    pred = sess.run(prediction ,{data: ix.reshape(1, seq_length, vocab)})\n",
    "    index = np.argmax(pred, 1)\n",
    "    \n",
    "    preds.append(ix_to_char[index[0]])\n",
    "    \n",
    "    zero_array = np.zeros(vocab)\n",
    "    zero_array[index] = 1\n",
    "    \n",
    "    tail = np.append(np.array(ix[0][1:]), np.array(zero_array.reshape(1, vocab)))\n",
    "    \n",
    "    ix = tail.reshape(1, seq_length, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raede, porque los\n",
      "demás dicirá menestertan el aquelle perdado de reyodo el in es on tiene bien merecido. Bien es verdad\n",
      "que yo soy hijotes, la quito el rucindados dica que los trueques, si es que tienes dellos.\n",
      "-Y que cien cacbbre.\n",
      "-Bespondió el galeote-, que yo tengo la receta de le pasaba del pecho; el cual, grado esto de las mercedes. ver, primerad malencio, y los estu caballos y dejarse ir por donde la vente la pada guar a se despedieme su pena de Paría temo tiene.\n",
      "\n",
      "Y aquí dio un sospiro, y le pasto de hombre de venerable rostro por se pareben presondé? De la cencalera, y\n",
      "no dar esposas avánerizo y de la Meigrre la pauendo puso a todos mis cinco\n",
      "sentidos de ser fengrundo, con la mesma cofradía. Pues, ¿qué será cuando me me mano pevarle que en meyor eso caeleventu, pidi al pasado con alta ni el cubar en ellosa mis, viata la intencia. De las mías entre vengan de los llevan, er algunos el caballero es\n",
      "genta, pero sinote de torar el de por la reja su señor a se eso. ¿udesa, decir a que\n"
     ]
    }
   ],
   "source": [
    "print(\"\".join(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample from the model now and then\n",
    "preds = []\n",
    "ix = X_test[144:145]\n",
    "\n",
    "for i in range(1000):\n",
    "    pred = sess.run(prediction ,{data: ix.reshape(1, seq_length, vocab)})\n",
    "    index = np.argmax(pred, 1)\n",
    "    \n",
    "    preds.append(ix_to_char[index[0]])\n",
    "    \n",
    "    zero_array = np.zeros(vocab)\n",
    "    zero_array[index] = 1\n",
    "    \n",
    "    tail = np.append(np.array(ix[0][1:]), np.array(zero_array.reshape(1, vocab)))\n",
    "    \n",
    "    ix = tail.reshape(1, seq_length, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "io: de todo el cuerpo, con gura tierta vencedo en pago de sus porque pena pongo detonda en buera en aHta, temgas, y tona y era ne el ente lespado a caballería quitarles los pon de no pudiero este le antes en\n",
      "la causa de su desgraci. palta, tengon ino con tuvolás caballe años. De lagaldo, como Der tare con el manifes de tode una\n",
      "doncella que no puede caballo, con\n",
      "escopetas de rueda, y allí a la gente be no se pues, salenca por te venir a ver en el sino en bueno el rucio!\n",
      "\n",
      "-Nunca yo acostunte ha tras sí la de saleve tienera con ellos la péndola del este camente eñ reiso tengo lo sía con esta entre el por vuescleros a pla ligen pare traen la quite -dijo don Quijote a mentaca tengo pigoa por tormento a las mil tras mía ser luevo se le saltiera en tus y y por le sabe de los lora cabala porevir a una ma pasanta\n",
      "por camino.\n",
      "\n",
      "-Y yo lo entiendo tuento estáve en ellos y ella pencada. Pues, peve entonos ve el rey, cuanto que venclao ve no dices balla el cabante como verces y grave tamente me caer\n"
     ]
    }
   ],
   "source": [
    "print(\"\".join(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved in file: ./quijote_model.ckpt\n"
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n",
    "save_path = saver.save(sess, \"./quijote_model.ckpt\")\n",
    "print(\"Model saved in file: %s\" % save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init_op = tf.global_variables_initializer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
